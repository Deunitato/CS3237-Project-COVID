{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = read_csv(filepath, header=None)\n",
    "    return dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a list of files into a 3D array of [samples, timesteps, features]\n",
    "def load_group(filenames, prefix=''):\n",
    "\tloaded = list()\n",
    "\tfor name in filenames:\n",
    "\t\tdata = load_file(prefix + name)\n",
    "\t\tloaded.append(data)\n",
    "\t# stack group so that features are the 3rd dimension\n",
    "\tloaded = dstack(loaded)\n",
    "\treturn loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "    # load all train\n",
    "    trainX, trainy = load_dataset_group('train', prefix + 'demo/')\n",
    "    print(trainX.shape, trainy.shape)\n",
    "    # load all test\n",
    "    testX, testy = load_dataset_group('test', prefix + 'demo/')\n",
    "    print(testX.shape, testy.shape)\n",
    "    # zero-offset class values\n",
    "#     trainy = trainy - 1\n",
    "#     testy = testy - 1\n",
    "    # one hot encode y\n",
    "    trainy = to_categorical(trainy)\n",
    "    testy = to_categorical(testy)\n",
    "    print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "    return trainX, trainy, testX, testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "    filepath = prefix + group + '/Inertial Signals/'\n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    # total acceleration\n",
    "    # \tfilenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "    # body acceleration\n",
    "#     filenames += ['body_acc_x_'+group+'.txt']\n",
    "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "    # body gyroscope\n",
    "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "    # load input data\n",
    "    X = load_group(filenames, filepath)\n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(trainX, trainy, testX, testy):\n",
    "    verbose, epochs, batch_size = 0, 10, 32\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    print(\"timestep: \",n_timesteps, \"n_features: \",n_features)\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "#     y_pred = model.predict(testX, batch_size=64, verbose=1)\n",
    "#     y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "#     print(testy)\n",
    "#     print(y_pred_bool)\n",
    "#     print(classification_report(testy, y_pred_bool))\n",
    "    # evaluate model\n",
    "#     _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    verbose, epochs, batch_size = 0, 10, 32\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "#     y_pred = model.predict(testX, batch_size=64, verbose=1)\n",
    "#     y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "#     print(testy)\n",
    "#     print(y_pred_bool)\n",
    "#     print(classification_report(testy, y_pred_bool))\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(scores):\n",
    "\tprint(scores)\n",
    "\tm, s = mean(scores), std(scores)\n",
    "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(repeats=10):\n",
    "    # load data\n",
    "    trainX, trainy, testX, testy = load_dataset()\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        score = evaluate_model(trainX, trainy, testX, testy)\n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r+1, score))\n",
    "        scores.append(score)\n",
    "    # summarize results\n",
    "    summarize_results(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel():\n",
    "    trainX, trainy, testX, testy = load_dataset()\n",
    "    model = create_model(trainX, trainy, testX, testy)\n",
    "    model.save('./modelTest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1100, 20, 6) (1100, 1)\n",
      "(268, 20, 6) (268, 1)\n",
      "(1100, 20, 6) (1100, 3) (268, 20, 6) (268, 3)\n",
      ">#1: 76.866\n",
      ">#2: 81.343\n",
      ">#3: 81.343\n",
      ">#4: 85.448\n",
      ">#5: 85.821\n",
      ">#6: 80.970\n",
      ">#7: 87.687\n",
      ">#8: 81.343\n",
      ">#9: 79.104\n",
      ">#10: 85.448\n",
      "[76.86567306518555, 81.34328126907349, 81.34328126907349, 85.447758436203, 85.8208954334259, 80.97015023231506, 87.68656849861145, 81.34328126907349, 79.10447716712952, 85.447758436203]\n",
      "Accuracy: 82.537% (+/-3.239)\n"
     ]
    }
   ],
   "source": [
    "run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1100, 20, 6) (1100, 1)\n",
      "(268, 20, 6) (268, 1)\n",
      "(1100, 20, 6) (1100, 3) (268, 20, 6) (268, 3)\n",
      "timestep:  20 n_features:  6\n",
      "WARNING:tensorflow:From c:\\users\\tianhang\\desktop\\cs3237\\gpu\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./modelTest\\assets\n"
     ]
    }
   ],
   "source": [
    "saveModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model2 = keras.models.load_model('./modelTest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1100, 20, 6) (1100, 1)\n",
      "(268, 20, 6) (268, 1)\n",
      "(1100, 20, 6) (1100, 3) (268, 20, 6) (268, 3)\n"
     ]
    }
   ],
   "source": [
    "trainX, trainy, testX, testy = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[1]\n",
      "[0]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[1]\n",
      "[1]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[2]\n",
      "[2]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for item in testX:\n",
    "    result = model2.predict(np.expand_dims(item, axis=0))\n",
    "#     print(item)\n",
    "#     print(result)\n",
    "    y = np.argmax(result, axis=-1)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 10, 6)\n"
     ]
    }
   ],
   "source": [
    "print(testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "[2 0 2 2 2 0 2 2 2 2 2 2 2 0 0 2 2 2 0 0 2 2 2 0 0 0 2 2 2 2 0 0 2 2 2 0 0\n",
      " 0 0 0 0 0 0 0 0 0 2 2 2 0 2 2 2 0 2 2 2 0 2 2 2 0 0 0 2 2 2 0 2 2 2 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 2 2 2 0 0 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(testy)\n",
    "y = np.argmax(testy, axis=-1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 7.1000e-01 -0.0000e+00 -4.8000e-01  1.1568e+02  9.9550e+01\n",
      "   -1.1691e+02]\n",
      "  [ 6.6000e-01 -2.8000e-01 -8.5000e-01  2.0120e+01  6.7490e+01\n",
      "   -8.1460e+01]\n",
      "  [ 3.2000e-01 -5.4000e-01 -1.0500e+00 -5.1020e+01 -2.3100e+00\n",
      "   -1.2130e+01]\n",
      "  [ 4.0000e-01  4.9000e-01 -5.8000e-01  1.4490e+01 -3.3210e+01\n",
      "    1.3490e+01]\n",
      "  [ 2.2000e-01 -6.4000e-01 -8.3000e-01 -8.5100e+00 -1.1450e+01\n",
      "    2.3580e+01]\n",
      "  [ 3.0000e-01 -6.1000e-01 -8.2000e-01 -2.1810e+01  4.1500e+00\n",
      "    1.0450e+01]\n",
      "  [ 3.0000e-01 -6.0000e-01 -8.1000e-01 -1.0930e+01  3.3000e-01\n",
      "   -3.2600e+00]\n",
      "  [ 3.0000e-01 -5.7000e-01 -7.4000e-01 -1.2685e+02  3.3100e+01\n",
      "    7.0140e+01]\n",
      "  [ 7.0000e-02 -5.4000e-01 -6.5000e-01  1.4316e+02 -8.2500e+01\n",
      "   -3.2190e+01]\n",
      "  [ 3.3000e-01 -5.2000e-01 -8.8000e-01  5.8290e+01  2.2000e-01\n",
      "    5.8300e+00]]\n",
      "\n",
      " [[ 3.0000e-01 -6.1000e-01 -8.2000e-01 -2.1810e+01  4.1500e+00\n",
      "    1.0450e+01]\n",
      "  [ 3.0000e-01 -6.0000e-01 -8.1000e-01 -1.0930e+01  3.3000e-01\n",
      "   -3.2600e+00]\n",
      "  [ 3.0000e-01 -5.7000e-01 -7.4000e-01 -1.2685e+02  3.3100e+01\n",
      "    7.0140e+01]\n",
      "  [ 7.0000e-02 -5.4000e-01 -6.5000e-01  1.4316e+02 -8.2500e+01\n",
      "   -3.2190e+01]\n",
      "  [ 3.3000e-01 -5.2000e-01 -8.8000e-01  5.8290e+01  2.2000e-01\n",
      "    5.8300e+00]\n",
      "  [ 3.3000e-01 -5.7000e-01 -8.3000e-01 -2.0050e+01 -1.0480e+01\n",
      "    1.3670e+01]\n",
      "  [ 1.5000e-01 -7.5000e-01 -8.1000e-01 -1.7130e+01 -4.1700e+00\n",
      "    1.0790e+01]\n",
      "  [ 1.5000e-01 -6.0000e-01 -7.9000e-01 -7.3400e+00  9.1500e+00\n",
      "   -1.4790e+01]\n",
      "  [ 2.6000e-01 -5.1000e-01 -8.2000e-01 -3.0610e+01  4.6040e+01\n",
      "   -6.5430e+01]\n",
      "  [ 2.6000e-01 -5.4000e-01 -8.2000e-01  6.8600e+00  5.3800e+00\n",
      "    1.1090e+01]]]\n"
     ]
    }
   ],
   "source": [
    "print(testX[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 7.1000e-01 -0.0000e+00 -4.8000e-01  1.1568e+02  9.9550e+01\n",
      "   -1.1691e+02]\n",
      "  [ 6.6000e-01 -2.8000e-01 -8.5000e-01  2.0120e+01  6.7490e+01\n",
      "   -8.1460e+01]\n",
      "  [ 3.2000e-01 -5.4000e-01 -1.0500e+00 -5.1020e+01 -2.3100e+00\n",
      "   -1.2130e+01]\n",
      "  [ 4.0000e-01  4.9000e-01 -5.8000e-01  1.4490e+01 -3.3210e+01\n",
      "    1.3490e+01]\n",
      "  [ 2.2000e-01 -6.4000e-01 -8.3000e-01 -8.5100e+00 -1.1450e+01\n",
      "    2.3580e+01]\n",
      "  [ 3.0000e-01 -6.1000e-01 -8.2000e-01 -2.1810e+01  4.1500e+00\n",
      "    1.0450e+01]\n",
      "  [ 3.0000e-01 -6.0000e-01 -8.1000e-01 -1.0930e+01  3.3000e-01\n",
      "   -3.2600e+00]\n",
      "  [ 3.0000e-01 -5.7000e-01 -7.4000e-01 -1.2685e+02  3.3100e+01\n",
      "    7.0140e+01]\n",
      "  [ 7.0000e-02 -5.4000e-01 -6.5000e-01  1.4316e+02 -8.2500e+01\n",
      "   -3.2190e+01]\n",
      "  [ 3.3000e-01 -5.2000e-01 -8.8000e-01  5.8290e+01  2.2000e-01\n",
      "    5.8300e+00]]]\n"
     ]
    }
   ],
   "source": [
    "print(np.expand_dims(testX[0], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9854666e-01 1.4209755e-12 1.4532653e-03]]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "result = model2.predict([[[ 7.1000e-01, -0.0000e+00 ,-4.8000e-01 , 1.1568e+02 , 9.9550e+01,\n",
    "   -1.1691e+02],\n",
    "  [ 6.6000e-01 ,-2.8000e-01 ,-8.5000e-01 , 2.0120e+01 , 6.7490e+01, -8.1460e+01],\n",
    "  [ 3.2000e-01, -5.4000e-01 ,-1.0500e+00, -5.1020e+01,-2.3100e+00,-1.2130e+01],\n",
    "  [ 4.0000e-01 , 4.9000e-01, -5.8000e-01 , 1.4490e+01, -3.3210e+01,\n",
    "    1.3490e+01],\n",
    "  [ 2.2000e-01, -6.4000e-01 ,-8.3000e-01 ,-8.5100e+00, -1.1450e+01,\n",
    "    2.3580e+01],\n",
    "  [ 3.0000e-01, -6.1000e-01, -8.2000e-01,-2.1810e+01,  4.1500e+00,\n",
    "    1.0450e+01],\n",
    "  [ 3.0000e-01, -6.0000e-01, -8.1000e-01, -1.0930e+01,  3.3000e-01,\n",
    "   -3.2600e+00],\n",
    "  [ 3.0000e-01, -5.7000e-01 ,-7.4000e-01, -1.2685e+02 , 3.3100e+01,\n",
    "    7.0140e+01],\n",
    "  [ 7.0000e-02 ,-5.4000e-01, -6.5000e-01 , 1.4316e+02,-8.2500e+01,\n",
    "   -3.2190e+01],\n",
    "  [ 3.3000e-01, -5.2000e-01, -8.8000e-01,  5.8290e+01 , 2.2000e-01,\n",
    "    5.8300e+00]]])\n",
    "print(result)\n",
    "y = np.argmax(result, axis=-1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.1000e-01 -0.0000e+00 -4.8000e-01  1.1568e+02  9.9550e+01 -1.1691e+02]\n",
      " [ 6.6000e-01 -2.8000e-01 -8.5000e-01  2.0120e+01  6.7490e+01 -8.1460e+01]\n",
      " [ 3.2000e-01 -5.4000e-01 -1.0500e+00 -5.1020e+01 -2.3100e+00 -1.2130e+01]\n",
      " [ 4.0000e-01  4.9000e-01 -5.8000e-01  1.4490e+01 -3.3210e+01  1.3490e+01]\n",
      " [ 2.2000e-01 -6.4000e-01 -8.3000e-01 -8.5100e+00 -1.1450e+01  2.3580e+01]\n",
      " [ 3.0000e-01 -6.1000e-01 -8.2000e-01 -2.1810e+01  4.1500e+00  1.0450e+01]\n",
      " [ 3.0000e-01 -6.0000e-01 -8.1000e-01 -1.0930e+01  3.3000e-01 -3.2600e+00]\n",
      " [ 3.0000e-01 -5.7000e-01 -7.4000e-01 -1.2685e+02  3.3100e+01  7.0140e+01]\n",
      " [ 7.0000e-02 -5.4000e-01 -6.5000e-01  1.4316e+02 -8.2500e+01 -3.2190e+01]\n",
      " [ 3.3000e-01 -5.2000e-01 -8.8000e-01  5.8290e+01  2.2000e-01  5.8300e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(testX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainy, testX, testy = load_dataset()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
